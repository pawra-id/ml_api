{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_ot7iZput-x",
        "outputId": "080be36d-33be-414b-c676-6fe498bf7b5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\azmik\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import ast\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rEMxhoNMvA05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\azmik\\Videos\\KULIAH\\FASTAPI\\ml_api\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsmLhViyu9Rn",
        "outputId": "98338d65-ebaa-490f-a643-bace9f842d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Sastrawi in c:\\users\\azmik\\videos\\kuliah\\fastapi\\ml_api\\venv\\lib\\site-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r6UpeuQWydtZ"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KR2kmMv10ejE"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPIRypk7uj6i"
      },
      "source": [
        "# Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EKoeFyH0uSqm"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "  text = re.sub('-',' ',text)\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  return text\n",
        "\n",
        "def case_folding(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "def tokenizingText(text):\n",
        "  text = word_tokenize(text)\n",
        "  return text\n",
        "\n",
        "def stemmingText(text):\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  text = [stemmer.stem(word) for word in text]\n",
        "  return text\n",
        "\n",
        "def toSentence(list_words): # Convert list of words into sentence\n",
        "  sentence = ' '.join(word for word in list_words)\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_SVDN07_vZXS"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(text):\n",
        "  cleaned_text = remove_punctuation(text)\n",
        "  cleaned_text = case_folding(cleaned_text)\n",
        "  tokenized_text = tokenizingText(cleaned_text)\n",
        "  stemmed_text = stemmingText(tokenized_text)\n",
        "  preprocessed_text = toSentence(stemmed_text)\n",
        "\n",
        "  return preprocessed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhrxBgXs7ZFH"
      },
      "source": [
        "# Tokenizing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_qDT0sExwoYf"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(split=' ', oov_token='<OOV>')\n",
        "with open('word_index.json') as json_word_index:\n",
        "  word_index = json.load(json_word_index)\n",
        "tokenizer.word_index = word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nlthNO95w849"
      },
      "outputs": [],
      "source": [
        "def token_for_sequences(preprocessed_text):\n",
        "  sequences = tokenizer.texts_to_sequences(preprocessed_text)\n",
        "  padded_sequences = pad_sequences(sequences, padding='post', maxlen=100)\n",
        "\n",
        "  return padded_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esh1OMdc7hNU"
      },
      "source": [
        "# Model and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "azwTC-ESyUmq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\azmik\\Videos\\KULIAH\\FASTAPI\\ml_api\\venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = load_model('bidirectional_lstm.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R7QFXjeUywdK"
      },
      "outputs": [],
      "source": [
        "def preprocess_to_predict(text):\n",
        "  preprocessed_text = data_preprocessing(text)\n",
        "  padded_sequences = token_for_sequences([preprocessed_text])\n",
        "  result = model.predict(padded_sequences)\n",
        "\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWQW3Ceazo16",
        "outputId": "aa922eef-df22-4bcd-8aec-6743123d27fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.44970438]], dtype=float32)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess_to_predict('Woofy agresif banget kalo lihat anjing lain. saya takut berantem sama anjing lain.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
